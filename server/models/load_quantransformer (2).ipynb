{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86XyBHVPj7q2",
        "outputId": "bdcff2df-d1bc-4192-8e58-141eabb72a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install --index-url https://download.pytorch.org/whl/cu126 \\\n",
        "torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Quantformer: per-ticker time split + weighted training + per-ticker metrics\n",
        "# ==========================================================\n",
        "import os, pickle, numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "metadata": {
        "id": "in2Ke3CIkwcP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "si44333dwDhx",
        "outputId": "94c15768-b994-4cba-f9c8-0aebe0e687b3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Load preprocessed dataset ----------\n",
        "load_path = \"/content/gdrive/MyDrive/sp500_quantformer_preprocessed.pkl\"  # <-- change if needed\n",
        "with open(load_path, \"rb\") as f:\n",
        "    data_bundle = pickle.load(f)\n",
        "\n",
        "X_all = data_bundle[\"X_all\"]                 # shape (N, seq_len, F)\n",
        "y_all = data_bundle[\"y_all\"]                 # continuous next-period returns (N,)\n",
        "sample_tickers = data_bundle[\"sample_tickers\"]  # per-sample ticker strings (N,)\n",
        "mu = data_bundle.get(\"mu\", None)             # optional (1, seq_len, F)\n",
        "sd = data_bundle.get(\"sd\", None)\n",
        "\n",
        "print(\"✅ Loaded:\", X_all.shape, y_all.shape, sample_tickers.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0BpvoiUk0p_",
        "outputId": "e6fe6acc-1717-4f4d-ce7d-8269fd248089"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded: (588886, 60, 1) (588886,) (588886,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Device ----------\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ---------- Labels: 3 equal bins on continuous returns ----------\n",
        "def label_three_bins(y):\n",
        "    y = np.asarray(y, dtype=float).reshape(-1)\n",
        "    q = np.quantile(y, [1/3, 2/3])\n",
        "    return np.digitize(y, q, right=True).astype(np.int64)\n",
        "\n",
        "y_cls = label_three_bins(y_all)\n",
        "num_bins = int(y_cls.max()) + 1\n",
        "print(\"Class balance:\", {int(k): int(v) for k, v in zip(*np.unique(y_cls, return_counts=True))})\n",
        "\n",
        "# ---------- Normalize (dataset-level) ----------\n",
        "X_used = X_all.astype(np.float32)\n",
        "if mu is not None and sd is not None:\n",
        "    X_used = (X_used - mu) / (sd + 1e-9)\n",
        "else:\n",
        "    mu = X_used.mean(axis=0, keepdims=True)\n",
        "    sd = X_used.std(axis=0, keepdims=True) + 1e-9\n",
        "    X_used = (X_used - mu) / sd\n",
        "\n",
        "assert np.isfinite(X_used).all(), \"NaN/Inf in normalized features\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZnDtmItk6Ar",
        "outputId": "e8bde4fe-3341-4ff1-8744-b6348ebe0a5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Class balance: {0: 196296, 1: 196295, 2: 196295}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Per-ticker chronological split (no leakage; each ticker contributes to val) ----------\n",
        "VAL_FRAC = 0.20\n",
        "idx_all  = np.arange(len(X_used))\n",
        "tick_all = np.asarray(sample_tickers)\n",
        "\n",
        "train_idx, val_idx = [], []\n",
        "for tkr in np.unique(tick_all):\n",
        "    m = (tick_all == tkr)\n",
        "    idx_t = idx_all[m]                 # chronological order by construction\n",
        "    n_t = len(idx_t)\n",
        "    if n_t < 5:                        # tiny series -> all in train\n",
        "        train_idx.extend(idx_t.tolist())\n",
        "        continue\n",
        "    cut = int((1.0 - VAL_FRAC) * n_t)\n",
        "    train_idx.extend(idx_t[:cut].tolist())\n",
        "    val_idx.extend(idx_t[cut:].tolist())\n",
        "\n",
        "train_idx = np.array(train_idx); train_idx.sort()\n",
        "val_idx   = np.array(val_idx);   val_idx.sort()\n",
        "\n",
        "X_tr, X_va = X_used[train_idx], X_used[val_idx]\n",
        "y_tr, y_va = y_cls[train_idx],  y_cls[val_idx]\n",
        "tickers_tr, tickers_va = tick_all[train_idx], tick_all[val_idx]\n",
        "y_va_cont = y_all[val_idx]  # continuous targets aligned to val\n",
        "\n",
        "print(f\"Train samples: {len(y_tr)} | Val samples: {len(y_va)}\")\n",
        "\n",
        "# ---------- Focus tickers & per-sample weights ----------\n",
        "focus_tickers = [\"TSLA\", \"AAPL\", \"NVDA\"]  # edit this list\n",
        "focus_tickers = [t.upper().strip() for t in focus_tickers]\n",
        "multiplier = 8.0                           # try 5–10\n",
        "\n",
        "weights = np.ones(len(y_tr), dtype=np.float32)\n",
        "for t in focus_tickers:\n",
        "    m = (np.char.upper(np.char.strip(tickers_tr.astype(str))) == t)\n",
        "    if m.any():\n",
        "        weights[m] *= multiplier\n",
        "weights /= weights.mean()\n",
        "\n",
        "print(\"Mean train weight:\", float(weights.mean()))\n",
        "for t in focus_tickers:\n",
        "    m = (np.char.upper(np.char.strip(tickers_tr.astype(str))) == t)\n",
        "    print(f\"{t}: train samples={int(m.sum())}, mean weight={float(weights[m].mean()) if m.any() else 'NA'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2ZnM316k-Wl",
        "outputId": "2762b84a-8e0b-44f3-a584-b128ffda9c54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 471104 | Val samples: 117782\n",
            "Mean train weight: 0.9999995827674866\n",
            "TSLA: train samples=956, mean weight=7.6730170249938965\n",
            "AAPL: train samples=956, mean weight=7.6730170249938965\n",
            "NVDA: train samples=956, mean weight=7.6730170249938965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- DataLoaders ----------\n",
        "X_tr_t = torch.tensor(X_tr, dtype=torch.float32)\n",
        "y_tr_t = torch.tensor(y_tr, dtype=torch.long)\n",
        "w_tr_t = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "X_va_t = torch.tensor(X_va, dtype=torch.float32)\n",
        "y_va_t = torch.tensor(y_va, dtype=torch.long)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(X_tr_t, y_tr_t, w_tr_t), batch_size=512, shuffle=True)\n",
        "val_loader   = DataLoader(TensorDataset(X_va_t, y_va_t), batch_size=1024, shuffle=False)\n",
        "\n",
        "# ---------- Model ----------\n",
        "class Quantformer(nn.Module):\n",
        "    def __init__(self, in_dim, num_bins, d_model=128, nhead=8, num_layers=4, dim_ff=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
        "            batch_first=True, norm_first=True, dropout=dropout, activation=\"gelu\"\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers)\n",
        "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_bins))\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(self.embed(x))   # (B, T, d)\n",
        "        return self.head(z.mean(1))       # mean-pool over time\n",
        "\n",
        "FEATS = X_used.shape[2]\n",
        "#model = Quantformer(in_dim=FEATS, num_bins=num_bins).to(device)\n",
        "model = Quantformer(\n",
        "    in_dim=FEATS, num_bins=num_bins,\n",
        "    d_model=192, nhead=12, num_layers=6,\n",
        "    dim_ff=384, dropout=0.25\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "veYC5yBAlDKY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b852a25-6b06-49f8-fc17-85dd8d323d5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Loss, Optimizer, Scheduler ----------\n",
        "counts = np.bincount(y_tr, minlength=num_bins).astype(np.float32)\n",
        "imbalance = (counts.min()/counts.max()) < 0.7\n",
        "if imbalance:\n",
        "    w_cls = counts.sum() / np.clip(counts, 1, None)\n",
        "    class_w = torch.tensor(w_cls / w_cls.mean(), dtype=torch.float32, device=device)\n",
        "    crit_base = nn.CrossEntropyLoss(weight=class_w, label_smoothing=0.03)\n",
        "else:\n",
        "    crit_base = nn.CrossEntropyLoss(label_smoothing=0.03)\n",
        "\n",
        "def weighted_ce(logits, targets, sample_w):\n",
        "    return (crit_base(logits, targets) * sample_w).mean()\n",
        "\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=40)"
      ],
      "metadata": {
        "id": "6fvnUwUylJRm"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- Training + Metrics ----------\n",
        "best = (1e9, None); patience=6; bad=0\n",
        "best_path = \"/content/gdrive/MyDrive/quantformer_best.pth\"\n",
        "\n",
        "# IC helper (optional)\n",
        "try:\n",
        "    from scipy.stats import spearmanr\n",
        "    def spearman_ic(a, b):\n",
        "        return spearmanr(a, b).correlation\n",
        "except Exception:\n",
        "    spearmanr = None\n",
        "    def spearman_ic(a, b):\n",
        "        return float(\"nan\")"
      ],
      "metadata": {
        "id": "_ezuYJG0kmwa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ep in range(1, 61):  # up to 60 epochs; early stopping will cut earlier\n",
        "    model.train(); tr_loss = 0.0\n",
        "    for xb, yb, wb in train_loader:\n",
        "        xb, yb, wb = xb.to(device), yb.to(device), wb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = model(xb)\n",
        "        loss = weighted_ce(logits, yb, wb)\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        opt.step()\n",
        "        tr_loss += loss.item() * xb.size(0)\n",
        "    tr_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval(); va_loss=0.0; correct=0; total=0\n",
        "    all_probs = []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            logits = model(xb)\n",
        "            p = torch.softmax(logits, 1)\n",
        "            va_loss += crit_base(logits, yb).item() * xb.size(0)\n",
        "            correct += (p.argmax(1) == yb).sum().item()\n",
        "            total += yb.numel()\n",
        "            all_probs.append(p.cpu().numpy())\n",
        "    va_loss /= len(val_loader.dataset)\n",
        "    va_acc  = correct / total\n",
        "    print(f\"Epoch {ep:02d} | train {tr_loss:.4f} | val {va_loss:.4f} | val-acc {va_acc:.3f}\")\n",
        "    sched.step()\n",
        "\n",
        "    # ---- Global metrics: IC + Precision@10% ----\n",
        "    all_probs = np.concatenate(all_probs, axis=0)    # (Nv, 3)\n",
        "    preds_va  = all_probs.argmax(1)\n",
        "    top_score = all_probs[:, 0]\n",
        "\n",
        "    # Align safely\n",
        "    Nv = len(top_score)\n",
        "    y_va_cont_aligned = y_va_cont[:Nv]\n",
        "    y_va_cls_aligned  = y_va[:Nv]\n",
        "    tickers_va_aligned = tickers_va[:Nv]\n",
        "\n",
        "    ic = spearman_ic(top_score, y_va_cont_aligned)\n",
        "    K = max(1, int(0.10 * Nv))\n",
        "    idx_top = np.argsort(-top_score)[:K]\n",
        "    prec_at_k = (y_va_cont_aligned[idx_top] > 0).mean()\n",
        "    print(f\"   · Val IC: {ic:.4f} | Precision@10%: {prec_at_k:.3f}\")\n",
        "\n",
        "    # ---- Per-ticker metrics (val-acc + P@10%) for focus tickers ----\n",
        "    tickers_va_norm = np.array([str(t).upper().strip() for t in tickers_va_aligned])\n",
        "    try:\n",
        "        import pandas as pd\n",
        "        cnt = pd.Series(tickers_va_norm).value_counts()\n",
        "        present = {t: int(cnt.get(t, 0)) for t in focus_tickers}\n",
        "        print(\"   · focus val counts:\", present)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    for t in focus_tickers:\n",
        "        m = (tickers_va_norm == t)\n",
        "        n_t = int(m.sum())\n",
        "        if n_t == 0:\n",
        "            print(f\"   · {t} val-acc: NA | P@10%: NA (n=0)\")\n",
        "            continue\n",
        "        acc_t = (preds_va[m] == y_va_cls_aligned[m]).mean()\n",
        "        Kt = max(1, int(0.10 * n_t))\n",
        "        idx_local = np.argsort(-top_score[m])[:Kt]\n",
        "        prec_t = (y_va_cont_aligned[m][idx_local] > 0).mean()\n",
        "        print(f\"   · {t} val-acc: {acc_t:.3f} | P@10%: {prec_t:.3f} (n={n_t})\")\n",
        "\n",
        "    # ---- Early stopping on val loss ----\n",
        "    if va_loss < best[0] - 1e-4:\n",
        "        best = (va_loss, {k: v.detach().cpu().clone() for k, v in model.state_dict().items()})\n",
        "        bad = 0\n",
        "        torch.save({\"state_dict\": best[1], \"mu\": mu, \"sd\": sd, \"focus_tickers\": focus_tickers},\n",
        "                   best_path)\n",
        "    else:\n",
        "        bad += 1\n",
        "        if bad >= patience:\n",
        "            print(f\"Early stopping at epoch {ep}\")\n",
        "            break\n",
        "\n",
        "# Restore best weights\n",
        "if best[1] is not None:\n",
        "    model.load_state_dict(best[1])\n",
        "    print(\"Loaded best checkpoint (lowest val loss). Saved to:\", best_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "vfaM_XLgvMIh",
        "outputId": "2d443079-aca1-445a-da90-460b580264fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | train 1.0861 | val 1.0910 | val-acc 0.377\n",
            "   · Val IC: 0.0237 | Precision@10%: 0.531\n",
            "   · focus val counts: {'TSLA': 239, 'AAPL': 239, 'NVDA': 239}\n",
            "   · TSLA val-acc: 0.477 | P@10%: 0.565 (n=239)\n",
            "   · AAPL val-acc: 0.414 | P@10%: 0.609 (n=239)\n",
            "   · NVDA val-acc: 0.372 | P@10%: 0.478 (n=239)\n",
            "Epoch 02 | train 1.0831 | val 1.0893 | val-acc 0.375\n",
            "   · Val IC: 0.0236 | Precision@10%: 0.531\n",
            "   · focus val counts: {'TSLA': 239, 'AAPL': 239, 'NVDA': 239}\n",
            "   · TSLA val-acc: 0.485 | P@10%: 0.522 (n=239)\n",
            "   · AAPL val-acc: 0.393 | P@10%: 0.609 (n=239)\n",
            "   · NVDA val-acc: 0.385 | P@10%: 0.522 (n=239)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1753634856.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtr_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Validation Backtest Simulator (per-ticker; base/global model)\n",
        "# - Strategy A: long-only when score in top q per ticker\n",
        "# - Strategy B: long-short (top q long, bottom q short) per ticker\n",
        "# - Optional isotonic calibration on TRAIN split (no leakage)\n",
        "# - Transaction costs in bps (round-trip modeled on entry/exit/flip)\n",
        "# Metrics: total return, CAGR* (approx), Sharpe, max drawdown\n",
        "# ==========================================================\n",
        "import os, pickle, numpy as np, torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.isotonic import IsotonicRegression\n",
        "\n",
        "# ---------- (A) Load preprocessed data if needed ----------\n",
        "# Comment this block if X_all etc. are already in memory\n",
        "DATA_PATH  = \"/content/gdrive/MyDrive/sp500_quantformer_preprocessed.pkl\"\n",
        "with open(DATA_PATH, \"rb\") as f:\n",
        "    data_bundle = pickle.load(f)\n",
        "X_all = data_bundle[\"X_all\"].astype(np.float32)\n",
        "y_all = data_bundle[\"y_all\"].astype(np.float32)          # continuous returns\n",
        "sample_tickers = data_bundle[\"sample_tickers\"]\n",
        "mu = data_bundle.get(\"mu\", None)\n",
        "sd = data_bundle.get(\"sd\", None)\n",
        "\n",
        "# Normalize exactly like training\n",
        "if mu is not None and sd is not None:\n",
        "    X_used = (X_all - mu) / (sd + 1e-9)\n",
        "else:\n",
        "    mu = X_all.mean(axis=0, keepdims=True)\n",
        "    sd = X_all.std(axis=0, keepdims=True) + 1e-9\n",
        "    X_used = (X_all - mu) / sd\n",
        "\n",
        "# ---------- (B) Model definition + load base checkpoint ----------\n",
        "class Quantformer(nn.Module):\n",
        "    def __init__(self, in_dim, num_bins, d_model=192, nhead=12, num_layers=6, dim_ff=384, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Linear(in_dim, d_model)\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
        "            batch_first=True, norm_first=True, dropout=dropout, activation=\"gelu\"\n",
        "        )\n",
        "        self.encoder = nn.TransformerEncoder(layer, num_layers)\n",
        "        self.head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, num_bins))\n",
        "    def forward(self, x):\n",
        "        z = self.encoder(self.embed(x))\n",
        "        return self.head(z.mean(1))\n",
        "\n",
        "def load_base_model(ckpt_path, in_dim, num_bins, device):\n",
        "    # PyTorch 2.6 safe-loading shim\n",
        "    from contextlib import suppress\n",
        "    with suppress(Exception):\n",
        "        import numpy as _np\n",
        "        torch.serialization.add_safe_globals([_np._core.multiarray._reconstruct])\n",
        "    try:\n",
        "        ckpt = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
        "    except Exception:\n",
        "        ckpt = torch.load(ckpt_path, map_location=device, weights_only=False)\n",
        "    state = ckpt[\"state_dict\"] if isinstance(ckpt, dict) and \"state_dict\" in ckpt else ckpt\n",
        "    model = Quantformer(in_dim, num_bins)\n",
        "    model.load_state_dict(state, strict=False)\n",
        "    return model\n",
        "\n",
        "CKPT_PATH = \"/content/gdrive/MyDrive/quantformer_best.pth\"   # <- global/base checkpoint\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ---------- (C) Helper: build global 3-class labels like training ----------\n",
        "def label_three_bins(y):\n",
        "    q = np.quantile(y, [1/3, 2/3])\n",
        "    return np.digitize(y, q, right=True).astype(np.int64)\n",
        "y_cls = label_three_bins(y_all)\n",
        "NUM_BINS = int(y_cls.max()) + 1\n",
        "FEATS = X_used.shape[2]\n",
        "\n",
        "# ---------- (D) Per-ticker chrono split (last 20% = validation) ----------\n",
        "def chrono_split_indices(tickers, target, val_frac=0.2):\n",
        "    t = np.asarray(tickers).astype(str)\n",
        "    m = (np.char.upper(np.char.strip(t)) == target.upper().strip())\n",
        "    idx = np.where(m)[0]\n",
        "    n = len(idx)\n",
        "    cut = max(1, int((1 - val_frac) * n))\n",
        "    return idx[:cut], idx[cut:]  # train_idx, val_idx\n",
        "\n",
        "# ---------- (E) Get model scores on arrays ----------\n",
        "@torch.no_grad()\n",
        "def predict_probs(model, X):\n",
        "    model.eval()\n",
        "    bs = 1024\n",
        "    out = []\n",
        "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
        "    for i in range(0, len(X_t), bs):\n",
        "        logits = model(X_t[i:i+bs])\n",
        "        out.append(torch.softmax(logits, dim=1).cpu().numpy())\n",
        "    return np.concatenate(out, axis=0)  # (N,3)\n",
        "\n",
        "# ---------- (F) Infer which class is UP/DOWN by mean return where class wins ----------\n",
        "def infer_up_down_ids(probs, y_cont):\n",
        "    pred = probs.argmax(1)\n",
        "    means = []\n",
        "    for c in range(probs.shape[1]):\n",
        "        m = (pred == c)\n",
        "        means.append((c, y_cont[m].mean() if m.any() else -1e9))\n",
        "    means.sort(key=lambda x: x[1])\n",
        "    down_id = means[0][0]\n",
        "    up_id   = means[-1][0]\n",
        "    return int(up_id), int(down_id)\n",
        "\n",
        "# ---------- (G) Backtest core (per ticker) ----------\n",
        "def backtest_per_ticker(returns, score, mode=\"long_only\", top_q=0.10, cost_bps=10):\n",
        "    \"\"\"\n",
        "    returns: (N,) simple daily returns for the ticker (validation segment)\n",
        "    score:   (N,) trading score aligned to returns (higher=more bullish)\n",
        "    mode:    \"long_only\" or \"long_short\"\n",
        "    top_q:   quantile for entries (e.g., 0.10 => top 10%)\n",
        "    cost_bps: per trade side, e.g., 10 bps (0.001). Round-trip applied on entry/exit/flip.\n",
        "    \"\"\"\n",
        "    N = len(returns)\n",
        "    thr_long = np.quantile(score, 1 - top_q)\n",
        "    if mode == \"long_only\":\n",
        "        pos = (score >= thr_long).astype(float)  # 1 or 0\n",
        "    elif mode == \"long_short\":\n",
        "        thr_short = np.quantile(score, top_q)\n",
        "        pos = np.zeros(N, dtype=float)\n",
        "        pos[score >= thr_long] = 1.0\n",
        "        pos[score <= thr_short] = -1.0\n",
        "    else:\n",
        "        raise ValueError(\"mode must be 'long_only' or 'long_short'\")\n",
        "\n",
        "    # Trades where position changes → apply costs\n",
        "    pos_prev = np.roll(pos, 1); pos_prev[0] = 0.0\n",
        "    trades = np.abs(pos - pos_prev)  # 1 on enter/exit, 2 on flip (-1->+1 or +1->-1)\n",
        "    # round-trip cost per change side:\n",
        "    # If you assume cost per side (enter or exit) = cost_bps, then cost per flip is 2*cost_bps\n",
        "    costs = trades * (cost_bps / 1e4)  # cost as fraction of notional\n",
        "\n",
        "    # PnL: position * return minus transaction costs (applied on change days)\n",
        "    pnl = pos * returns - costs\n",
        "    equity = (1 + pnl).cumprod()\n",
        "    total_ret = equity[-1] - 1.0\n",
        "\n",
        "    # Metrics\n",
        "    avg_daily = pnl.mean()\n",
        "    vol_daily = pnl.std(ddof=1) + 1e-12\n",
        "    sharpe = np.sqrt(252) * avg_daily / vol_daily  # daily → annualized\n",
        "    # Max drawdown\n",
        "    roll_max = np.maximum.accumulate(equity)\n",
        "    drawdown = equity / roll_max - 1.0\n",
        "    mdd = drawdown.min()\n",
        "\n",
        "    # Approx CAGR (no dates; assume 252 trading days/year; len=N)\n",
        "    years = max(len(pnl) / 252.0, 1e-9)\n",
        "    cagr = equity[-1] ** (1.0 / years) - 1.0\n",
        "\n",
        "    return {\n",
        "        \"equity\": equity,\n",
        "        \"pnl\": pnl,\n",
        "        \"total_return\": float(total_ret),\n",
        "        \"cagr\": float(cagr),\n",
        "        \"sharpe\": float(sharpe),\n",
        "        \"max_drawdown\": float(mdd),\n",
        "        \"trades\": int((trades > 0).sum())\n",
        "    }\n",
        "\n",
        "# ---------- (H) Run the simulation ----------\n",
        "def simulate_validation(\n",
        "    tickers=(\"TSLA\",\"AAPL\",\"NVDA\"),\n",
        "    ckpt_path=CKPT_PATH,\n",
        "    mode=\"long_only\",           # \"long_only\" or \"long_short\"\n",
        "    top_q=0.10,                 # top quantile for entries\n",
        "    cost_bps=10,                # per-side costs in bps\n",
        "    use_isotonic=True           # calibrate score on TRAIN, apply on VAL\n",
        "):\n",
        "    model = load_base_model(ckpt_path, FEATS, NUM_BINS, device).to(device)\n",
        "    results = {}\n",
        "    total_pnl = []\n",
        "\n",
        "    for tkr in tickers:\n",
        "        tr_idx, va_idx = chrono_split_indices(sample_tickers, tkr, val_frac=0.2)\n",
        "        if len(va_idx) == 0:\n",
        "            print(f\"{tkr}: no validation data; skipping.\")\n",
        "            continue\n",
        "\n",
        "        Xtr, ytr_cont = X_used[tr_idx], y_all[tr_idx]\n",
        "        Xva, yva_cont = X_used[va_idx], y_all[va_idx]\n",
        "\n",
        "        # Model probabilities\n",
        "        probs_tr = predict_probs(model, Xtr)\n",
        "        probs_va = predict_probs(model, Xva)\n",
        "\n",
        "        # Infer UP/DOWN ids on TRAIN; score = margin on VAL\n",
        "        up_id, down_id = infer_up_down_ids(probs_tr, ytr_cont)\n",
        "        margin_tr = probs_tr[:, up_id] - probs_tr[:, down_id]\n",
        "        margin_va = probs_va[:, up_id] - probs_va[:, down_id]\n",
        "\n",
        "        # Optional isotonic calibration (fit on TRAIN only; apply to VAL)\n",
        "        if use_isotonic:\n",
        "            order = np.argsort(margin_tr)\n",
        "            ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
        "            ir.fit(margin_tr[order], ytr_cont[order])\n",
        "            score_va = ir.predict(margin_va)\n",
        "        else:\n",
        "            score_va = margin_va\n",
        "\n",
        "        # Backtest per ticker\n",
        "        bt = backtest_per_ticker(\n",
        "            returns=yva_cont, score=score_va,\n",
        "            mode=mode, top_q=top_q, cost_bps=cost_bps\n",
        "        )\n",
        "        results[tkr] = bt\n",
        "        total_pnl.append(bt[\"pnl\"])\n",
        "\n",
        "        print(f\"[{tkr}] mode={mode}, top_q={top_q}, cost={cost_bps}bps  \"\n",
        "              f\"Ret={bt['total_return']:.2%}  CAGR≈{bt['cagr']:.2%}  \"\n",
        "              f\"Sharpe={bt['sharpe']:.2f}  MDD={bt['max_drawdown']:.2%}  Trades={bt['trades']}\")\n",
        "\n",
        "    # Aggregate across tickers (equal notional per ticker)\n",
        "    if total_pnl:\n",
        "        pnl_all = np.vstack(total_pnl).mean(axis=0)  # equal-weight across tickers\n",
        "        equity = (1 + pnl_all).cumprod()\n",
        "        total_ret = equity[-1] - 1.0\n",
        "        avg_daily = pnl_all.mean()\n",
        "        vol_daily = pnl_all.std(ddof=1) + 1e-12\n",
        "        sharpe = np.sqrt(252) * avg_daily / vol_daily\n",
        "        roll_max = np.maximum.accumulate(equity)\n",
        "        mdd = (equity / roll_max - 1.0).min()\n",
        "        years = max(len(pnl_all) / 252.0, 1e-9)\n",
        "        cagr = equity[-1] ** (1.0 / years) - 1.0\n",
        "\n",
        "        print(f\"\\n[AGG] Equal-weighted across tickers → \"\n",
        "              f\"Ret={total_ret:.2%}  CAGR≈{cagr:.2%}  Sharpe={sharpe:.2f}  MDD={mdd:.2%}\")\n",
        "        results[\"_AGG_\"] = {\n",
        "            \"total_return\": float(total_ret),\n",
        "            \"cagr\": float(cagr),\n",
        "            \"sharpe\": float(sharpe),\n",
        "            \"max_drawdown\": float(mdd),\n",
        "            \"equity\": equity,\n",
        "            \"pnl\": pnl_all\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# ---------- (I) Run some scenarios ----------\n",
        "# 1) Long-only, top 10%, 10 bps costs\n",
        "res1 = simulate_validation(\n",
        "    tickers=(\"TSLA\",\"AAPL\",\"NVDA\"),\n",
        "    ckpt_path=CKPT_PATH,\n",
        "    mode=\"long_only\",\n",
        "    top_q=0.10,\n",
        "    cost_bps=10,\n",
        "    use_isotonic=True\n",
        ")\n",
        "\n",
        "# 2) Long-short, 10% tails, 10 bps costs\n",
        "res2 = simulate_validation(\n",
        "    tickers=(\"TSLA\",\"AAPL\",\"NVDA\"),\n",
        "    ckpt_path=CKPT_PATH,\n",
        "    mode=\"long_short\",\n",
        "    top_q=0.10,\n",
        "    cost_bps=10,\n",
        "    use_isotonic=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUz-X-Pbrgj6",
        "outputId": "a45f839d-4c17-45f9-b01c-640d2d1e0458"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TSLA] mode=long_only, top_q=0.1, cost=10bps  Ret=127.00%  CAGR≈137.36%  Sharpe=2.51  MDD=-11.53%  Trades=11\n",
            "[AAPL] mode=long_only, top_q=0.1, cost=10bps  Ret=16.88%  CAGR≈17.88%  Sharpe=0.93  MDD=-18.65%  Trades=3\n",
            "[NVDA] mode=long_only, top_q=0.1, cost=10bps  Ret=13.22%  CAGR≈13.99%  Sharpe=0.52  MDD=-41.14%  Trades=5\n",
            "\n",
            "[AGG] Equal-weighted across tickers → Ret=50.20%  CAGR≈53.56%  Sharpe=2.00  MDD=-17.82%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TSLA] mode=long_short, top_q=0.1, cost=10bps  Ret=182.86%  CAGR≈199.32%  Sharpe=1.96  MDD=-43.43%  Trades=12\n",
            "[AAPL] mode=long_short, top_q=0.1, cost=10bps  Ret=18.57%  CAGR≈19.67%  Sharpe=0.71  MDD=-22.89%  Trades=3\n",
            "[NVDA] mode=long_short, top_q=0.1, cost=10bps  Ret=4.88%  CAGR≈5.16%  Sharpe=0.35  MDD=-41.14%  Trades=5\n",
            "\n",
            "[AGG] Equal-weighted across tickers → Ret=68.15%  CAGR≈72.97%  Sharpe=2.31  MDD=-11.36%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ygi7mRzltHB-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}